# Robots.txt - Prevent all bot access
User-agent: *
Disallow: /

# Prevent specific bots
User-agent: Googlebot
Disallow: /

User-agent: Bingbot
Disallow: /

User-agent: Slurp
Disallow: /

User-agent: DuckDuckBot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: YandexBot
Disallow: /

User-agent: facebookexternalhit
Disallow: /

User-agent: Twitterbot
Disallow: /

User-agent: LinkedInBot
Disallow: /

User-agent: WhatsApp
Disallow: /

# Prevent crawling of all paths
Disallow: /src/
Disallow: /api/
Disallow: /admin/
Disallow: /login/
Disallow: /dashboard/

# Crawl-delay to slow down any bots that might ignore the rules
Crawl-delay: 10
